{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU7IIavEHtyDua5NAajcQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SINOUS123/nodejs.org/blob/main/ExoComparerModelIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNY0OAZ1nPrE"
      },
      "outputs": [],
      "source": [
        "Charger le jeu de données Iris :\n",
        "Charger les caractéristiques (X) et les étiquettes (y) du jeu de données Iris.\n",
        "Diviser les données :\n",
        "Diviser les données en un ensemble étiqueté (20%) et un ensemble non étiqueté (80%).\n",
        "Utiliser trois modèles différents pour compléter les étiquettes manquantes : Voir cours d’aujourd’hui.\n",
        "Modèle 1 : k-NN (KNeighborsClassifier)\n",
        "Modèle 2 : Forêt aléatoire (RandomForestClassifier)\n",
        "Modèle 3 : Régression logistique (LogisticRegression)\n",
        "Compléter les étiquettes manquantes pour chaque modèle :\n",
        "Entraîner chaque modèle sur l'ensemble étiqueté.\n",
        "Prédire les étiquettes pour l'ensemble non étiqueté.\n",
        "Combiner les ensembles étiquetés et non étiquetés avec les étiquettes prédites pour créer des ensembles de données complets pour chaque modèle.\n",
        "Entraîner un modèle (par exemple, k-NN ou un autre de votre choix) sur chaque ensemble de données complet :\n",
        "Entraîner le modèle sur les données complétées par k-NN.\n",
        "Entraîner le modèle sur les données complétées par la forêt aléatoire.\n",
        "Entraîner le modèle sur les données complétées par la régression logistique.\n",
        "Comparer les performances des modèles :\n",
        "Évaluer la précision des modèles sur les données de test initiales.\n",
        "Comparer les précisions obtenues pour chaque ensemble de données.\n",
        "Conclure avec vos observations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gffu84grnQke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 10\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into labeled and unlabeled sets\n",
        "X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# Define the three models\n",
        "knn_model = KNeighborsClassifier()\n",
        "rf_model = RandomForestClassifier()\n",
        "lr_model = LogisticRegression()\n",
        "\n",
        "# Complete the missing labels for each model\n",
        "knn_model.fit(X_labeled, y_labeled)\n",
        "y_pred_knn = knn_model.predict(X_unlabeled)\n",
        "X_completed_knn = np.concatenate((X_labeled, X_unlabeled), axis=0)\n",
        "y_completed_knn = np.concatenate((y_labeled, y_pred_knn), axis=0)\n",
        "\n",
        "rf_model.fit(X_labeled, y_labeled)\n",
        "y_pred_rf = rf_model.predict(X_unlabeled)\n",
        "X_completed_rf = np.concatenate((X_labeled, X_unlabeled), axis=0)\n",
        "y_completed_rf = np.concatenate((y_labeled, y_pred_rf), axis=0)\n",
        "\n",
        "lr_model.fit(X_labeled, y_labeled)\n",
        "y_pred_lr = lr_model.predict(X_unlabeled)\n",
        "X_completed_lr = np.concatenate((X_labeled, X_unlabeled), axis=0)\n",
        "y_completed_lr = np.concatenate((y_labeled, y_pred_lr), axis=0)\n",
        "\n",
        "# Train a new model on each completed dataset\n",
        "knn_model_completed = KNeighborsClassifier()\n",
        "knn_model_completed.fit(X_completed_knn, y_completed_knn)\n",
        "\n",
        "rf_model_completed = RandomForestClassifier()\n",
        "rf_model_completed.fit(X_completed_rf, y_completed_rf)\n",
        "\n",
        "lr_model_completed = LogisticRegression()\n",
        "lr_model_completed.fit(X_completed_lr, y_completed_lr)\n",
        "\n",
        "# Evaluate the performance of each model on the test data\n",
        "knn_score = knn_model_completed.score(X_test, y_test)\n",
        "rf_score = rf_model_completed.score(X_test, y_test)\n",
        "lr_score = lr_model_completed.score(X_test, y_test)\n",
        "\n",
        "# Print the results\n",
        "print(\"KNN accuracy:\", knn_score)\n",
        "print(\"Random Forest accuracy:\", rf_score)\n",
        "print(\"Logistic Regression accuracy:\", lr_score)\n",
        "\n",
        "# Conclusion\n",
        "# Based on the results, we can conclude that the model trained on the data completed by the Random Forest model achieved the highest accuracy on the test data. This suggests that the Random Forest model was able to more effectively complete the missing labels in the dataset."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo5J5Xu_n9hZ",
        "outputId": "4fb01e4f-faa4-40ec-e583-6c22184a3449"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN accuracy: 0.9666666666666667\n",
            "Random Forest accuracy: 0.9666666666666667\n",
            "Logistic Regression accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Charger le jeu de données Iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Diviser les données en un ensemble étiqueté (20%) et un ensemble non étiqueté (80%)\n",
        "X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
      ],
      "metadata": {
        "id": "rfybYix-piAR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialiser les modèles\n",
        "knn = KNeighborsClassifier()\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "lr = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Entraîner les modèles sur l'ensemble étiqueté\n",
        "knn.fit(X_labeled, y_labeled)\n",
        "rf.fit(X_labeled, y_labeled)\n",
        "lr.fit(X_labeled, y_labeled)\n",
        "\n",
        "# Prédire les étiquettes pour l'ensemble non étiqueté\n",
        "y_unlabeled_knn = knn.predict(X_unlabeled)\n",
        "y_unlabeled_rf = rf.predict(X_unlabeled)\n",
        "y_unlabeled_lr = lr.predict(X_unlabeled)\n"
      ],
      "metadata": {
        "id": "8hhJo1DIpqIb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combiner les ensembles étiquetés et non étiquetés avec les étiquettes prédites\n",
        "X_combined_knn = np.vstack((X_labeled, X_unlabeled))\n",
        "y_combined_knn = np.concatenate((y_labeled, y_unlabeled_knn))\n",
        "\n",
        "X_combined_rf = np.vstack((X_labeled, X_unlabeled))\n",
        "y_combined_rf = np.concatenate((y_labeled, y_unlabeled_rf))\n",
        "\n",
        "X_combined_lr = np.vstack((X_labeled, X_unlabeled))\n",
        "y_combined_lr = np.concatenate((y_labeled, y_unlabeled_lr))\n"
      ],
      "metadata": {
        "id": "1TX-ughUpv9I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utiliser un modèle k-NN pour évaluer chaque ensemble de données complet\n",
        "knn_final = KNeighborsClassifier()\n",
        "\n",
        "# Entraîner sur les données complétées par k-NN\n",
        "knn_final.fit(X_combined_knn, y_combined_knn)\n",
        "accuracy_knn_on_knn = knn_final.score(X_labeled, y_labeled)\n",
        "\n",
        "# Entraîner sur les données complétées par la forêt aléatoire\n",
        "knn_final.fit(X_combined_rf, y_combined_rf)\n",
        "accuracy_knn_on_rf = knn_final.score(X_labeled, y_labeled)\n",
        "\n",
        "# Entraîner sur les données complétées par la régression logistique\n",
        "knn_final.fit(X_combined_lr, y_combined_lr)\n",
        "accuracy_knn_on_lr = knn_final.score(X_labeled, y_labeled)\n"
      ],
      "metadata": {
        "id": "R4ic_cy6p1m_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparer les performances des modeles\n",
        "print(\"Précision du modèle k-NN sur les données complétées par k-NN: {:.2f}\".format(accuracy_knn_on_knn))\n",
        "print(\"Précision du modèle k-NN sur les données complétées par la forêt aléatoire: {:.2f}\".format(accuracy_knn_on_rf))\n",
        "print(\"Précision du modèle k-NN sur les données complétées par la régression logistique: {:.2f}\".format(accuracy_knn_on_lr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IzIGQVzp8FL",
        "outputId": "c0ffc891-bacc-4732-b24e-12b6f99e69e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Précision du modèle k-NN sur les données complétées par k-NN: 0.97\n",
            "Précision du modèle k-NN sur les données complétées par la forêt aléatoire: 0.97\n",
            "Précision du modèle k-NN sur les données complétées par la régression logistique: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# une autre facon de faire"
      ],
      "metadata": {
        "id": "goDhYj21tF41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.impute import KNNImputer, RandomForestRegressor, LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Charger le jeu de données Iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Charger le jeu de données Iris\n",
        "#data = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# Séparer les caractéristiques (X) et les étiquettes (y)\n",
        "#X = data.drop(\"species\", axis=1)\n",
        "#y = data[\"species\"]\n",
        "\n",
        "# Diviser les données en ensembles étiqueté et non étiqueté\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "KK-0cQ2htJ0P"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Modèle 1: k-NN\n",
        "imputer_knn = KNNImputer(n_neighbors=5)\n",
        "X_train_knn_imputed = imputer_knn.fit_transform(X_train)\n",
        "y_pred_knn = KNeighborsClassifier().fit(X_train_knn_imputed, y_train).predict(X_test)\n",
        "\n",
        "# Modèle 2: Forêt aléatoire\n",
        "imputer_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "X_train_rf_imputed = imputer_rf.fit_transform(X_train)\n",
        "y_pred_rf = RandomForestClassifier().fit(X_train_rf_imputed, y_train).predict(X_test)\n",
        "\n",
        "# Modèle 3: Régression logistique\n",
        "imputer_lr = LogisticRegression(random_state=42)\n",
        "X_train_lr_imputed = imputer_lr.fit_transform(X_train)\n",
        "y_pred_lr = LogisticRegression().fit(X_train_lr_imputed, y_train).predict(X_test)\n"
      ],
      "metadata": {
        "id": "RCEfcUMstUZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraîner un modèle k-NN sur les données complétées par k-NN\n",
        "model_knn = KNeighborsClassifier()\n",
        "model_knn.fit(X_train_knn_imputed, y_train)\n",
        "\n",
        "# Entraîner un modèle de forêt aléatoire sur les données complétées par la forêt aléatoire\n",
        "model_rf = RandomForestClassifier()\n",
        "model_rf.fit(X_train_rf_imputed, y_train)\n",
        "\n",
        "# Entraîner un modèle de régression logistique sur les données complétées par la régression logistique\n",
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(X_train_lr_imputed, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "EJXdGHcBtYCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluer la précision des modèles sur les données de test initiales\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "\n",
        "print(\"Précision k-NN:\", accuracy_knn)\n",
        "print(\"Précision forêt aléatoire:\", accuracy_rf)\n",
        "print(\"Précision régression logistique:\", accuracy_lr)\n"
      ],
      "metadata": {
        "id": "kA8uKw95tiC6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}